{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-11T13:30:38.612764800Z",
     "start_time": "2025-02-11T13:30:32.508599500Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import torch\n",
    "import numpy as np\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_enable_fp32_cpu_offload=True,\n",
    "    # llm_int8_has_fp16_weight=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-11T13:30:38.630716800Z",
     "start_time": "2025-02-11T13:30:38.615204700Z"
    }
   },
   "id": "7388b01a2a2d8578"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8836a0a803ed4c92b60547fb8c1add23"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct\", device_map=\"cuda\", quantization_config=quantization_config)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-11T13:31:02.313050400Z",
     "start_time": "2025-02-11T13:30:38.618722500Z"
    }
   },
   "id": "f2fd29275e4ebc01"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-11T13:32:25.814081300Z",
     "start_time": "2025-02-11T13:32:25.810073300Z"
    }
   },
   "id": "20268ea2f3e7b1fe"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-11T13:33:35.099871200Z",
     "start_time": "2025-02-11T13:33:35.086244500Z"
    }
   },
   "id": "ebaae521994d2fea"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-11T13:33:47.782696800Z",
     "start_time": "2025-02-11T13:33:47.755718900Z"
    }
   },
   "id": "11804c43dcb5894c"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-11T13:35:28.485470400Z",
     "start_time": "2025-02-11T13:34:05.205390600Z"
    }
   },
   "id": "f28b56e81c798af3"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-11T13:35:28.502515200Z",
     "start_time": "2025-02-11T13:35:28.486470400Z"
    }
   },
   "id": "5d10344664f0861e"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! A large language model (LLM) is a type of artificial intelligence system designed to understand and generate human-like text. These models are typically based on deep learning techniques, particularly transformer architectures, which allow them to process and understand vast amounts of textual data.\n",
      "\n",
      "LLMs are trained on massive datasets containing a wide variety of text from the internet, books, articles, and more. This extensive training enables them to learn complex patterns in language, allowing them to perform various natural language processing tasks such as:\n",
      "\n",
      "- **Text generation**: Creating coherent paragraphs or conversations.\n",
      "- **Translation**: Translating text from one language to another.\n",
      "- **Summarization**: Generating concise summaries of longer texts.\n",
      "- **Question answering**: Providing answers to questions based on given information.\n",
      "- **Text classification**: Categorizing text into predefined categories.\n",
      "\n",
      "Some well-known examples of LLMs include models like GPT (Generative Pre-trained Transformer), BERT (Bidirectional Encoder Representations from Transformers), and T5 (Text-to-Text Transfer Transformer). These models have become increasingly powerful and versatile, driving advancements in fields ranging from customer service chatbots to content creation and beyond.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-11T13:36:32.454741100Z",
     "start_time": "2025-02-11T13:36:32.449222800Z"
    }
   },
   "id": "a3c07bfb12a9c9ac"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "import os\n",
    "from getpass import getpass"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T21:39:24.435989200Z",
     "start_time": "2025-02-03T21:39:24.397243Z"
    }
   },
   "id": "c590f9a95faf8bf5"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "response = requests.get('https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt')\n",
    "text = response.text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T21:39:25.803789500Z",
     "start_time": "2025-02-03T21:39:25.596327500Z"
    }
   },
   "id": "d5b5e45a8dfa39b0"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "37"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_size = 2048\n",
    "chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "len(chunks)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T21:39:43.668326Z",
     "start_time": "2025-02-03T21:39:43.661746100Z"
    }
   },
   "id": "564f8219cc13e1c8"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def get_text_embedding(input_text):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    last_hidden_state = outputs.last_hidden_state\n",
    "    \n",
    "    embedding = last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    \n",
    "    return embedding\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T21:43:46.460634400Z",
     "start_time": "2025-02-03T21:43:46.455393100Z"
    }
   },
   "id": "c8eee1b2fc6ed787"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CausalLMOutputWithPast' object has no attribute 'last_hidden_state'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m chunks \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThis is the first chunk.\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThis is the second chunk.\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m----> 2\u001B[0m text_embeddings \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(\u001B[43m[\u001B[49m\u001B[43mget_text_embedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunks\u001B[49m\u001B[43m]\u001B[49m)\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(text_embeddings)\n",
      "Cell \u001B[1;32mIn[9], line 2\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m      1\u001B[0m chunks \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThis is the first chunk.\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThis is the second chunk.\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m----> 2\u001B[0m text_embeddings \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray([\u001B[43mget_text_embedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m chunks])\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(text_embeddings)\n",
      "Cell \u001B[1;32mIn[8], line 7\u001B[0m, in \u001B[0;36mget_text_embedding\u001B[1;34m(input_text)\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m      5\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m model(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minputs)\n\u001B[1;32m----> 7\u001B[0m last_hidden_state \u001B[38;5;241m=\u001B[39m \u001B[43moutputs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlast_hidden_state\u001B[49m\n\u001B[0;32m      9\u001B[0m embedding \u001B[38;5;241m=\u001B[39m last_hidden_state\u001B[38;5;241m.\u001B[39mmean(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39msqueeze()\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m embedding\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'CausalLMOutputWithPast' object has no attribute 'last_hidden_state'"
     ]
    }
   ],
   "source": [
    "chunks = [\"This is the first chunk.\", \"This is the second chunk.\"]\n",
    "text_embeddings = np.array([get_text_embedding(chunk) for chunk in chunks])\n",
    "\n",
    "print(text_embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T21:45:10.873096900Z",
     "start_time": "2025-02-03T21:43:46.706836500Z"
    }
   },
   "id": "a66c43612baed9d9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3e9c8721e7de3784"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
