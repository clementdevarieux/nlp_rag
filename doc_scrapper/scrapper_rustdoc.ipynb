{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-17T09:07:31.083062Z",
     "start_time": "2025-01-17T09:07:31.079810Z"
    }
   },
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T09:07:31.268414Z",
     "start_time": "2025-01-17T09:07:31.265287Z"
    }
   },
   "cell_type": "code",
   "source": [
    "BASE_URL = \"https://docs.rs\"\n",
    "CRATE_URL = \"https://docs.rs/burn/latest/burn/\""
   ],
   "id": "a1615ccdbceb49de",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T09:38:23.175398Z",
     "start_time": "2025-01-17T09:38:23.170152Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def to_markdown(html_content):\n",
    "    \"\"\"\n",
    "    Convertit rapidement (et de façon rudimentaire) un contenu HTML en Markdown.\n",
    "    Pour quelque chose de plus élaboré, on peut envisager l'utilisation\n",
    "    de bibliothèques comme html2text, ou l’écriture de règles plus fines.\n",
    "    \"\"\"\n",
    "    # On peut faire qq conversions de base, par exemple remplacer <h1>, <h2>... \n",
    "    # et enlever les balises de code, etc.\n",
    "    # Ici, on fait quelque chose de très minimaliste :\n",
    "    text = re.sub(r'<h1[^>]*>(.*?)</h1>', r'# \\1', html_content, flags=re.DOTALL)\n",
    "    text = re.sub(r'<h2[^>]*>(.*?)</h2>', r'## \\1', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'<h3[^>]*>(.*?)</h3>', r'### \\1', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'<code[^>]*>(.*?)</code>', r'`\\1`', text, flags=re.DOTALL)\n",
    "    # Supprimer toutes les balises HTML restantes\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    # Retirer les espaces multiples\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "    text = text.strip()\n",
    "    return text"
   ],
   "id": "dcfd7f29b7724c4b",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T09:38:23.344465Z",
     "start_time": "2025-01-17T09:38:23.341119Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_soup(url):\n",
    "    \"\"\"Retourne le BeautifulSoup d'une URL.\"\"\"\n",
    "    r = requests.get(url)\n",
    "    r.raise_for_status()\n",
    "    return BeautifulSoup(r.text, 'html.parser')"
   ],
   "id": "525d87323e4380bd",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T09:42:08.459617Z",
     "start_time": "2025-01-17T09:42:08.455395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_main_content(soup):\n",
    "    main_div = soup.find(id=\"main\")\n",
    "    if not main_div:\n",
    "        main_div = soup.body.find(id=\"main-content\")\n",
    "\n",
    "    # Supprimer la navigation, barres latérales ou tout autre élément non-essentiel\n",
    "    for nav_tag in main_div.find_all([\"nav\", \"aside\", \"rustdoc-search\", \"copy-path\", \"rustdoc-toolbar\"]):\n",
    "        nav_tag.decompose()\n",
    "\n",
    "    # Par exemple, supprimer des disclaimers ou sections non-requises\n",
    "    disclaimers = main_div.find_all(class_=\"disclaimer\")\n",
    "    for d in disclaimers:\n",
    "        d.decompose()\n",
    "\n",
    "    return str(main_div)"
   ],
   "id": "d2443d5fb83d37b",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T09:42:08.678021Z",
     "start_time": "2025-01-17T09:42:08.673064Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_doc_links(soup):\n",
    "    \"\"\"\n",
    "    Récupère les liens internes importants (modules, traits, macros, structs, etc.).\n",
    "    Ils apparaissent généralement dans la <section class='sidebar-elems'> \n",
    "    ou bien dans des <a> avec un href commençant par './' ou similaire.\n",
    "    \"\"\"\n",
    "    doc_links = set()\n",
    "    \n",
    "    # Exemples de selecteurs possibles. \n",
    "    # On pourrait aussi parcourir toutes les balises <a> et filtrer.\n",
    "    sidebar = soup.find(\"section\", class_=\"sidebar-elems\")\n",
    "    if sidebar:\n",
    "        for a in sidebar.find_all(\"a\", href=True):\n",
    "            href = a[\"href\"]\n",
    "            # On prend les liens internes à la doc (souvent \"./\" ou \"../trait...\")\n",
    "            if href.startswith(\"./\") or href.startswith(\"../\"):\n",
    "                doc_links.add(href)\n",
    "    \n",
    "    # Conversion en liens absolus\n",
    "    absolute_links = []\n",
    "    for link in doc_links:\n",
    "        if link.startswith(\"./\"):\n",
    "            absolute_links.append(CRATE_URL + link[2:])  # retire \"./\" et ajoute base\n",
    "        elif link.startswith(\"../\"):\n",
    "            # remontée d'un niveau : on retire '../' et on concatène\n",
    "            # Cependant, il peut y avoir plusieurs niveaux. À ajuster si besoin.\n",
    "            # Ex : \"../foo\" => ndarray/foo\n",
    "            link_part = link.lstrip(\"./\")\n",
    "            absolute_links.append(os.path.join(CRATE_URL, link_part))\n",
    "        else:\n",
    "            # Autres cas, potentiellement des liens complets\n",
    "            if link.startswith(\"http\"):\n",
    "                absolute_links.append(link)\n",
    "            else:\n",
    "                absolute_links.append(CRATE_URL + link)\n",
    "                \n",
    "    return list(set(absolute_links))"
   ],
   "id": "d4cee9337bd16932",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T09:42:08.789029Z",
     "start_time": "2025-01-17T09:42:08.784016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def scrape_ndarray_docs():\n",
    "    # 1. Récupérer la page d'accueil\n",
    "    main_soup = get_soup(CRATE_URL)\n",
    "    \n",
    "    # 2. Récupérer le contenu principal\n",
    "    main_content_html = extract_main_content(main_soup)\n",
    "    \n",
    "    # 3. Conversion en Markdown\n",
    "    main_content_md = to_markdown(main_content_html)\n",
    "    \n",
    "    # 4. Récupérer la liste des liens internes\n",
    "    links = get_doc_links(main_soup)\n",
    "    print(links)\n",
    "    \n",
    "    all_docs_md = main_content_md + \"\\n\\n\"\n",
    "    \n",
    "    visited = set()  # pour éviter de scraper plusieurs fois la même page\n",
    "    for link in links:\n",
    "        if link not in visited:\n",
    "            visited.add(link)\n",
    "            try:\n",
    "                page_soup = get_soup(link)\n",
    "                page_content_html = extract_main_content(page_soup)\n",
    "                page_content_md = to_markdown(page_content_html)\n",
    "                \n",
    "                all_docs_md += f\"## Page : {link}\\n\\n\"\n",
    "                all_docs_md += page_content_md + \"\\n\\n\"\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Impossible de scraper {link} : {e}\")\n",
    "    \n",
    "    # 6. Sauvegarder le tout dans un fichier Markdown\n",
    "    with open(\"ndarray_doc.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(all_docs_md)\n"
   ],
   "id": "ab7efae425ff1477",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T09:42:08.967988Z",
     "start_time": "2025-01-17T09:42:08.902979Z"
    }
   },
   "cell_type": "code",
   "source": "scrape_ndarray_docs()",
   "id": "bcd72e48b2b28989",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T09:42:09.113744Z",
     "start_time": "2025-01-17T09:42:09.111087Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "5675a6459747b6d0",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ada7b1d779fc8581"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
